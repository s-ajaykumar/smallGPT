{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c6a819-2559-4e91-89f0-c8a7cc661811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/outbell/Ajay/DeepLearning/GenAI/smallGPT/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import vocab\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c9be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/outbell/Ajay/DeepLearning/GenAI/smallGPT/data/shakesphere.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "processed_data = re.findall(r'\\S+,?\\s+|\\S+,|\\S+\\s+', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a305755a-3d95-4bf0-b688-369cd786434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    vocab_size = len(vocab.itos) \n",
    "    n_embd = 2\n",
    "    n_hidden = 4*n_embd\n",
    "    n_heads = 1\n",
    "    n_layers = 1\n",
    "    c_block_size = 15\n",
    "    w_block_size = 10\n",
    "    dropout_ratio = 0.2\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e77113-3e51-4b17-8408-049bd537b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention cpu version\n",
    "\n",
    "class CharAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
    "        self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, W, c, C = x.shape\n",
    "        \n",
    "        qkv = self.attn(x)\n",
    "        q, k, v = self.qkv.split(config.n_embd, dim = -1)\n",
    "        q = q.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        k = k.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        v = v.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        out = out.transpose(2, 3).contiguous().view(B, W, c, C)\n",
    "        \n",
    "        out = self.c_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        out = x + out             # Residual connection\n",
    "        out = out[:, :, -1, :]    # B, W, C\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
    "        self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, W, C = x.shape\n",
    "        \n",
    "        qkv = self.attn(x)\n",
    "        q, k, v = self.qkv.split(config.n_embd, dim = -1)\n",
    "        q = q.view(B, W, config.n_heads, C//config.n_heads).transpose(1, 2)\n",
    "        k = k.view(B, W, config.n_heads, C//config.n_heads).transpose(1, 2)\n",
    "        v = v.view(B, W, config.n_heads, C//config.n_heads).transpose(1, 2)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, W, C)\n",
    "        \n",
    "        out = self.c_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_hidden, bias = False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.n_hidden, config.n_embd, bias = False),\n",
    "            nn.Dropout(config.dropout_ratio),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.w_attn = WordAttention()\n",
    "        self.mlp = MLP()\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.w_attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.cpe = nn.Embedding(config.c_block_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.w_block_size, config.n_embd)\n",
    "        \n",
    "        self.c_attn = CharAttention()\n",
    "        self.h = nn.ModuleList([Block() for _ in range(config.n_layers)])\n",
    "        self.lm_heads = nn.ModuleList([nn.Linear(config.n_hidden, config.vocab_size) for _ in range(config.c_block_size)])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c_emb = self.cte(x)\n",
    "        c_pos_emb = self.cpe(x)\n",
    "        x = c_emb + c_pos_emb\n",
    "        x = self.c_attn(x)\n",
    "        pos_emb = self.wpe(torch.arange(x.shape[1], dtype = torch.long, device = config.device))\n",
    "        x = x + pos_emb\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        logits = []\n",
    "        for lm_head in self.lm_heads:\n",
    "            logits.append(lm_head(x))\n",
    "        logits = torch.stack(logits, dim = 2)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17df9b9-a14c-4615-bc36-1cf8f33b2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention cpu version\n",
    "\n",
    "class CharSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
    "        self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, W, c, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = self.qkv.split(config.n_embd, dim = -1)\n",
    "        q = q.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        k = k.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        v = v.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        out = out.transpose(2, 3).contiguous().view(B, W, c, C)\n",
    "        out = self.c_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_hidden, bias = False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.n_hidden, config.n_embd, bias = False)\n",
    "        self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        word = x[-1]\n",
    "        print(\"Forward input:\", word, word.shape)\n",
    "        word = self.c_fc(x)\n",
    "        word = self.gelu(x)\n",
    "        word = self.c_proj(x)\n",
    "        x = torch.cat((x[:-1], word), dim = 0)\n",
    "        print(\"forward output: \", x, x.shape)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CharSelfAttention()\n",
    "        self.mlp = MLP()\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.cte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.cpe = nn.Embedding(config.c_block_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.w_block_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, chs in enumerate(x):\n",
    "            c_emb = self.cte(chs)\n",
    "            c_pos_emb = self.cpe(chs)\n",
    "            chs = c_emb + c_pos_emb\n",
    "            if 'any of the end tokens of a word' == chs[-1]:\n",
    "                w_pos_emb = self.wpe(i)\n",
    "                chs[-1] += w_pos_emb\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c3215-f276-400b-9b4c-84d31bed113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention GPU version\n",
    "\n",
    "class CharSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, W, T, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = self.qkv.split(config.n_embd, dim = -1)\n",
    "        q = q.view(B, W, T, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        k = k.view(B, W, T, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        v = v.view(B, W, T, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        out = out.transpose(2, 3).contiguous().view(B, W, T, C)\n",
    "        out = self.c_proj(out)\n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_hidden, bias = False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.n_hidden, config.n_embd, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        word = x[-1]\n",
    "        print(\"Forward input:\", word, word.shape)\n",
    "        word = self.c_fc(x)\n",
    "        word = self.gelu(x)\n",
    "        word = self.c_proj(x)\n",
    "        x = torch.cat((x[:-1], word), dim = 0)\n",
    "        print(\"forward output: \", x, x.shape)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.\n",
    "        \n",
    "        \n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.cte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.cpe = nn.Embedding(config.c_block_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.w_block_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, chs in enumerate(x):\n",
    "            char_emb = self.cte(chs)\n",
    "            char_pos_emb = self.cpe(chs)\n",
    "            word_pos_emb = self.wpe(i)\n",
    "            chs = char_emb + char_pos_emb\n",
    "            chs[-1] += word_pos_emb\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
