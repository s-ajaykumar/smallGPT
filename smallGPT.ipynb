{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c6a819-2559-4e91-89f0-c8a7cc661811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9ec6a4-57c8-4c84-a6ec-32348429cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {\n",
    "    0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j',\n",
    "    10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't',\n",
    "    20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z',\n",
    "\n",
    "    26: 'A', 27: 'B', 28: 'C', 29: 'D', 30: 'E', 31: 'F', 32: 'G', 33: 'H', 34: 'I', 35: 'J',\n",
    "    36: 'K', 37: 'L', 38: 'M', 39: 'N', 40: 'O', 41: 'P', 42: 'Q', 43: 'R', 44: 'S', 45: 'T',\n",
    "    46: 'U', 47: 'V', 48: 'W', 49: 'X', 50: 'Y', 51: 'Z',\n",
    "\n",
    "    52: '0', 53: '1', 54: '2', 55: '3', 56: '4', 57: '5', 58: '6', 59: '7', 60: '8', 61: '9',\n",
    "\n",
    "    62: '.', 63: ',', 64: ';', 65: ':', 66: '?', 67: '!', 68: \"'\", 69: '\"',\n",
    "    70: '-', 71: '(', 72: ')', 73: '[', 74: ']', 75: '{', 76: '}',\n",
    "\n",
    "    77: ' ', 78: '\\n', 79: '\\t',\n",
    "\n",
    "    80: '@', 81: '#', 82: '$', 83: '%', 84: '^', 85: '&', 86: '*', 87: '_',\n",
    "    88: '+', 89: '=', 90: '/', 91: '\\\\', 92: '|', 93: '~', 94: '`',\n",
    "    95: '<', 96: '>', 97: '–', 98: '—'\n",
    "}\n",
    "stoi = {\n",
    "    'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9,\n",
    "    'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19,\n",
    "    'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25,\n",
    "\n",
    "    'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35,\n",
    "    'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45,\n",
    "    'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51,\n",
    "\n",
    "    '0': 52, '1': 53, '2': 54, '3': 55, '4': 56, '5': 57, '6': 58, '7': 59, '8': 60, '9': 61,\n",
    "\n",
    "    '.': 62, ',': 63, ';': 64, ':': 65, '?': 66, '!': 67, \"'\": 68, '\"': 69,\n",
    "    '-': 70, '(': 71, ')': 72, '[': 73, ']': 74, '{': 75, '}': 76,\n",
    "\n",
    "    ' ': 77, '\\n': 78, '\\t': 79,\n",
    "\n",
    "    '@': 80, '#': 81, '$': 82, '%': 83, '^': 84, '&': 85, '*': 86, '_': 87,\n",
    "    '+': 88, '=': 89, '/': 90, '\\\\': 91, '|': 92, '~': 93, '`': 94,\n",
    "    '<': 95, '>': 96, '–': 97, '—': 98\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4cc451-6002-40c7-aca5-fbe94d75e0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a305755a-3d95-4bf0-b688-369cd786434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = len(itos) \n",
    "    n_embd = 2\n",
    "    n_hidden = 4*n_embd\n",
    "    n_heads = 1\n",
    "    n_layers = 1\n",
    "    c_block_size = 15\n",
    "    w_block_size = 10\n",
    "    dropout_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e77113-3e51-4b17-8408-049bd537b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention cpu version\n",
    "\n",
    "class CharAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
    "        self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, W, c, C = x.shape\n",
    "        \n",
    "        qkv = self.attn(x)\n",
    "        q, k, v = self.qkv.split(config.n_embd, dim = -1)\n",
    "        q = q.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        k = k.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        v = v.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        out = out.transpose(2, 3).contiguous().view(B, W, c, C)\n",
    "        \n",
    "        out = self.c_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        out = x + out             # Residual connection\n",
    "        out = out[:, :, -1, :]    # B, W, C\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
    "        self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, W, C = x.shape\n",
    "        \n",
    "        qkv = self.attn(x)\n",
    "        q, k, v = self.qkv.split(config.n_embd, dim = -1)\n",
    "        q = q.view(B, W, config.n_heads, C//config.n_heads).transpose(1, 2)\n",
    "        k = k.view(B, W, config.n_heads, C//config.n_heads).transpose(1, 2)\n",
    "        v = v.view(B, W, config.n_heads, C//config.n_heads).transpose(1, 2)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, W, C)\n",
    "        \n",
    "        out = self.c_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self.c_fc = nn.Linear(config.n_embd, config.n_hidden, bias = False)\n",
    "            self.gelu = nn.GELU()\n",
    "            self.c_proj = nn.Linear(config.n_hidden, config.n_embd, bias = False)\n",
    "            self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.w_attn = WordAttention()\n",
    "        self.mlp = MLP()\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.w_attn(self.ln_1(out))\n",
    "        x = x + self.mlp(self.ln_2(out))\n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.cpe = nn.Embedding(config.c_block_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.w_block_size, config.n_embd)\n",
    "        \n",
    "        self.c_attn = CharAttention()\n",
    "        self.h = nn.ModuleList([Block() for _ in range(config.n_layers)])\n",
    "        self.lm_heads = nn.ModuleList([nn.Linear(config.n_hidden, config.vocab_size) for _ in range(config.c_block_size)])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c_emb = self.cte(x)\n",
    "        c_pos_emb = self.cpe(chs)\n",
    "        x = c_emb + c_pos_emb\n",
    "        x = self.c_attn(chs)\n",
    "        pos_emb = self.wpe(torch.arange(x.shape[1], dtype = torch.long, device = config.device))\n",
    "        x = x + pos_emb\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        logits = []\n",
    "        for lm_head in self.lm_heads:\n",
    "            logits.append(lm_head(x))\n",
    "        logits = torch.stack(logits, dim = 2)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17df9b9-a14c-4615-bc36-1cf8f33b2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention cpu version\n",
    "\n",
    "class CharSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
    "        self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, W, c, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = self.qkv.split(config.n_embd, dim = -1)\n",
    "        q = q.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        k = k.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        v = v.view(B, W, c, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        out = out.transpose(2, 3).contiguous().view(B, W, c, C)\n",
    "        out = self.c_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_hidden, bias = False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.n_hidden, config.n_embd, bias = False)\n",
    "        self.dropout = nn.Dropout(config.dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        word = x[-1]\n",
    "        print(\"Forward input:\", word, word.shape)\n",
    "        word = self.c_fc(x)\n",
    "        word = self.gelu(x)\n",
    "        word = self.c_proj(x)\n",
    "        x = torch.cat((x[:-1], word), dim = 0)\n",
    "        print(\"forward output: \", x, x.shape)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CharSelfAttention()\n",
    "        self.mlp = MLP()\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.cte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.cpe = nn.Embedding(config.c_block_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.w_block_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, chs in enumerate(x):\n",
    "            c_emb = self.cte(chs)\n",
    "            c_pos_emb = self.cpe(chs)\n",
    "            chs = c_emb + c_pos_emb\n",
    "            if 'any of the end tokens of a word' == chs[-1]:\n",
    "                w_pos_emb = self.wpe(i)\n",
    "                chs[-1] += w_pos_emb\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c3215-f276-400b-9b4c-84d31bed113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention GPU version\n",
    "\n",
    "class CharSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = False)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, W, T, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = self.qkv.split(config.n_embd, dim = -1)\n",
    "        q = q.view(B, W, T, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        k = k.view(B, W, T, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        v = v.view(B, W, T, config.n_heads, C//config.n_heads).transpose(2, 3)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        out = out.transpose(2, 3).contiguous().view(B, W, T, C)\n",
    "        out = self.c_proj(out)\n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_hidden, bias = False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.n_hidden, config.n_embd, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        word = x[-1]\n",
    "        print(\"Forward input:\", word, word.shape)\n",
    "        word = self.c_fc(x)\n",
    "        word = self.gelu(x)\n",
    "        word = self.c_proj(x)\n",
    "        x = torch.cat((x[:-1], word), dim = 0)\n",
    "        print(\"forward output: \", x, x.shape)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.\n",
    "        \n",
    "        \n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.cte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.cpe = nn.Embedding(config.c_block_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.w_block_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, chs in enumerate(x):\n",
    "            char_emb = self.cte(chs)\n",
    "            char_pos_emb = self.cpe(chs)\n",
    "            word_pos_emb = self.wpe(i)\n",
    "            chs = char_emb + char_pos_emb\n",
    "            chs[-1] += word_pos_emb\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
